# Data-Parallel Volume Path Tracing (with Ray Forwarding)

This project aims at enabling the data-parallel rendering of (so
far-)structured volumetric data on one or more ranks with one or more
GPUs. Note we currently assume each GPU will be used through its own
MPI rank, even if there are multiple GPUs in the same physical nodes.

Below some very brief information on building VOPAT; followed by some
brief description of what one needs to do to get a data set
converted/imported into vopat until it can be renderered, as well as
on how to control the viewer once a model is running.

# Building

Vopat (or VoPaT, for VOlumePAthTracing) is built with CMake. You need
CUDA, a relatively recent NVIDIA driver, Cmake, and MPI. For the
viewers you'll also want to have QT installed, as well as glfw.

## Prerequisites

- libraries: QT, glfw, possibly netcdf for the ncToRaw tool (see below)
- CUDA, version 11 should do
- OptiX, any version of 7.0
- CUDA Aware MPI: We use OpenMPI 4.1.2, configured and built with ``--with-cuda` flag; but any other CUDA aware MPI should do, too.

## Source Dependencies

All source dependencies come int he form of git submodules. Do initial
close with `--recursive` flag, or, if you forgot that, do a `git
submodule init; git submodule update`.

## Building w/ CMake

build w/ cmake as usual; currently no extra build flags to be
mentioned here. When configuring cmake assumes that MPI and CUDA
(nvcc) are in the path; OWL may also require to find OptiX, for which
you can set a `OptiX_INSTALL_DIR` environment or cmdline variable.

Once built, you should have binaries `vopatQtViewer`, `vopatSplitter`,
and, if enabled during build, a `ncToRaw` tool.

## Build on Frontera

Building on frontera requires requesting a rtx-dev since it requires mvapich2-gdr/2.3.5.

On a rtx-dev node load modules cuda/11.0 and mvapich2-gdr/2.3.5. (Note mpi cuda aware currently only works with cuda 11.0 on frontera). Use the instructions above to build.  


# Getting vopat to render a volume

## NetCDF to raw conversion

The vopat splitter can currently only read 'raw' volume data sets. To
convert form (uncompressed float) netcdf you can use the `ncToRaw`
tool as follows:

    ./ncToRaw mkow030-bigdomain-uncompressed.00252000.nc -o /space/mkow
	
Two notes:

a) The value specified with `-o` is only the *prefix* of the generated
file name; the tool will automatically append dimension, dtatype, and
`.raw` extension.

b) this tools is disabled in cmake by default (to avoid the
`libnetcdf` dependency).  If you have those libs installed you have to
enable it through cmake.

## Splitting a model into "bricks"

Though the vopat *renderer* can probably also be attached to data
provided through other means, the current viewers require some special
`.vopat` file format that contains a structured volume that has been
split into `N` "bricks". To split a given model (e.g, `magnetic.raw`), use this:

    ./vopatSplitter /path/to/magnetic.raw -n 8 -if float -is 512 512 512 -o /nfs/magnetic-n8 

Explanation of parameters:

- `-n <int>` number of bricks to split this into. This should be the desired number of worker ranks.
- `-is <int> <int> <int>` "input size" (ie dimensions) of the input volume in the given raw file.

- `-if float|uint16|uint8` "input format" (scalar type of the input
  model). Float models get read as in, uint8 and uint16 get normalized
  to [0,1] values.
  
- `-o <path>` base prefix of all files generated by the splitter; this
  will be one `<path>.vopat` with the metadata, and several
  `<path>/...brick` files for the generated bricks (so each rank needs
  to load only its brick).

## MPI-Launching the vopat viewer

In this section I assume you split your model into `/nfs/myModel`
(which means there should be a `/nfs/myModel.vopat` file, as well as
several `/nfs/myModel...brick` files). I also assume that this folder
is nfs-shared readable by all the ranks you intend to start (though it
is also possible to use local folders, as long as every rank can open
the respective files).

*Number of ranks:* You always *have* to use one more rank than you have bricks
in your model; rank 0 will always run the viewer, window, transfer
function editor, etc, as well as the "master" controlling the workers;
ranks 1..N will be workers.

*MPI host list:* Multiple GPUs on the same node are used by listing
the same rank with several MPI processses on the same rank; Vopat will
automatically figure out if there's multiple processes on the same
node, and will use those GPUs round robin. 


### Example: Running on own cluster with OpenMPI: 

E.g., on my host cluster (with display/master machine `envy` and four
dual-GPU workers `moggy`, `shady`, `hasky`, and `wally`, I would
launch an 8-way split model using

    export HOSTS=envy,wally:2,shady:2,hasky:2,moggy:2
    /home/wald/opt/bin/mpirun -n 9 -host $HOSTS /mnt/nfs/vopatQTViewer /mnt/nfs/vopat/qi8
	
Note the above assumes a CUDA-aware build of OpenMPI (ie *not* the
default linux install of it!), that `/mnt/nfs` is mounted and
accessible on all ranks, and that `/home/wald/opt/bin` is the local
install dir for openmpi (and acessible to all ranks).
	
### Example: Running on Frontera

Frontera provides 4 NVidia GPU's per node. For best performance allocate for MPI tasks per requested node to avoid GPU oversubscription. Request a DCV job, which provides a better graphical support than VNC.

```
sbatch -A <Allocation> -N <#Number of Nodes> -n <#Number of nodes * 4> /share/doc/slurm/job.dcv
```

Inside the DCV section
```
ml load cuda/11.0 mvapich2-gdr/2.3.5
export LD_PRELOAD=/opt/apps/intel19/mvapich2-gdr/2.3.5/lib64/libmpi.so

ibrun ./vopatQTViewer <data location>
```







# Controlling the Viewer (once Lauched)

## Camera movement

Camera motion is controlled by dragging the mouse in the main display window: 

- `right button drag` : move in/out

- `middle button drag`: "strafe" sideways

- `left button drag`: rotate either around the lookat point (if in
  "inspect" mode), or around the camera position (if in "fly" mode)

Note all camera motion is influenced by the "up direction", which can
be different for each model, but which can be controlled by the
(uppercase) `X`, `Y`, and `Z` keystrokes). Camera motion is also
sligthly different based on inspect mode vs fly mode (which be
switched via pressing `F` or `I`), and by the "motion speed" (which
can be controlled via the `+`/`-` keys.

## Important Keystrokes

### Keystrokes to control camera position / motion

- `X`,`Y`,`Z` (uppercase): switch up direction ("upvector") to +/-
  X,Y, or Z direction (pressing twice inverts up/down).

- `I` : switch to "inspect" mode (rotate around the current lookat point)

- 'F' : "fly mode" with no lookat point; free flight around the entire
  model, and camera rotation around the camera position

- `+`/`-`: increase/decrease motion speed (how much you move for any mouse tick) by 1.5x

### Other useful keystrokes

- `C`: print current camera position on terminal, in a form that can be copy-n-pasted
   on next run
   
- `!`: dump some screenshot of current position (possibly includgin some
  extra images for per-rank partial frame buffers)
  
- `@`: dump current transfer function to "vopat.xf" (allows for using same xf on next run)

  

## Transfer Function Editor

In the main editor field, you can "draw" with left mouse button to
modify the opacity value; setting the opacity value to match the
current mounse position. Drawing with the *right* mouse button works
the same, but sets opacity of that bin to either 0 or 1, whichever is
closer.

In the edit fields/spin boxes:

- `opacity scale` allows for modifying the "density" of the volume, on
  a logarithmic scale. "100" is a (relatively high) default density;
  reducing this value will make the volume more transparent,
  increasing it will make it more opaque.
  
- `abs domain` allows for clamping the tranfer function to a (sub)
  region of the model's input data values. I.e., if the input values
  go from -1 to +10, but the abs domain fields specify 0 and 2, then
  the used transfer fucntion will actually only affect data values
  from 0 to 2, with all values below or above that range clamped into
  that range.
  
- `rel domain`: same as the abs domain, but (percentage-)relative to
  the abs domain. Ie, a rel domain of [0,100] means the tranfer
  function domain is exactly the abs domain described in the previous
  bullet; a rel domain of [25,50] for a abs domain of [0,10] would
  mean the actual domain of the transfer function is [2.5,5] (ie, 25
  percent and 50 percent inside the [0,10] range).



TODO

- getting to work on TACC: then also get to run on large data (dns?
  cloud or llnl animatied?)

- optimization: have all ranks exhcnage their macrocell info, then
  have 'getNextNode()' use this to try and skip the next rank if
  possible: if currnet rank knows what random number the next ray
  would use for that rank, and has the same macrocells, AND tracing
  the ray through the next ray's macrocells doesn't yeild a single
  scatter event, then that next rank can be skipped w/ needing to
  forward the ray!
  
- optimization: partial frame buffers could probably be sent w/ lower
  bandwidth if we look at 8x8 tiles and send only those that aren't
  entirely empty.
  
- visualizations: woodock w/ ray forwarding, woodcock with *local*
  only shadows (no forwarding fo shadow rays), as emission absorption
  only.
  
- clouds: done' tuse transfer function, but do PHOTOREAL cloud

- iso-surfaces

- clouds - animation?

- multiple samples per pixel (maybe w/ control from user)

- CUDA hardware textures :-/

- POSSIBLY rendering directly on zfp compressed data, OR at least
  upload zfp and decompress.
  
- offline rendering, with high-res and/or high sample count

